{"./":{"url":"./","title":"介绍","keywords":"","body":"【编程技巧】 《代码大全》 【编程语言】 《Java 核心技术（卷 1）》 《Head First Java》 《Spring in Action》 《Spring Boot 实战》 【操作系统】 《鸟哥的 Linux 私房菜》 【网络协议】 MDN：HTTP 文档 了解 HTTP 头、HTTP 请求方法、HTTP 返回码 了解 Cookie、缓存、会话、连接管理 【数据库设计】 Imooc：数据库设计的那些事 《MySQL 必知必会》 MySQL 官方文档 【字符编码】 Blog：关于字符编码，你所需要知道的) Blog：The history of Character Encoding Wiki：Character encoding GitHub：Awesome Unicode GitHub：Awesome Code Points 【编程工具】 Intellij IDEA Blog：Intellij IDEA 中文教程 Git Blog：Pro Git 第二版 Blog：猴子都能懂的 Git 入门 GitHub：GitHub and Git 图文教程 Blog：Git 图文教程及详解 Chrome Blog：超完整的 Chrome 浏览器客户端调试大全 powered by Gitbook修订时间： 2023-04-03 14:53:24 "},"markdown/mysql/standard/guide.html":{"url":"markdown/mysql/standard/guide.html","title":"快速指引","keywords":"","body":"你想做些什么？ 我想新建一张表 我需要为已有表增加或修改字段 我需要为表建立或删除索引 我需要备份数据 我要修改线上数据 我正在开发业务功能 我想新建一张表 > 建表语句参考 CREATE TABLE `table_name` ( `id` INT UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '主键', `column_name1` TINYINT UNSIGNED NOT NULL DEFAULT 0 COMMENT 'tinyint 示例', `column_name2` INT UNSIGNED NOT NULL COMMENT 'int 示例', `column_name3` VARCHAR(100) NOT NULL DEFAULT '' COMMENT 'varchar 示例', `column_name4` DECIMAL(16,2) NOT NULL DEFAULT 0.00 COMMENT 'decimal 示例', `column_name5` DATETIME NOT NULL COMMENT 'datetime 示例', `is_deleted` TINYINT UNSIGNED NOT NULL DEFAULT 0 COMMENT '逻辑删除 (1：已删，0：未删)', `gmt_create` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `gmt_modified` DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间', PRIMARY KEY (`id`), UNIQUE KEY `uk_column_name` (`column_name`), KEY `idx_column_name` (`column_name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='表备注'; > 需要注意的事项 参考：建表需要注意什么 参考：如何命名 参考：如何选择列的类型 参考：如何建立索引 我需要为已有表增加或修改字段 > 需要注意的事项 参考：如何命名？ 参考：如何选择列的类型？ 参考：如何建立索引？ 修改字段时，将多个 ALTER 语句合并为一个执行 所有表和字段都要添加注释，修改字段含义或对字段表示的状态追加时，需要及时更新字段注释 我需要为表建立或删除索引 > 建立索引语句参考 ALTER TABLE table_name ADD INDEX idx_column1_column2(`column1`,`column2`); ALTER TABLE table_name ADD UNIQUE uk_column1_column2(`column1`,`column2`); > 删除索引语句参考 ALTER TABLE table_name DROP INDEX idx_xxx; > 需要注意的事项 参考：如何建立索引？ 我需要备份数据 > 备份语句参考 CREATE TABLE bak_table_name_yyyyMMdd LIKE table_name; ALTER TABLE bak_table_name_yyyyMMdd COMMENT 'xxxx备份，请保留至yyyyMMdd'; INSERT INTO bak_table_name_yyyyMMdd SELECT * FROM table_name [WHERE ...]; > 需要注意的事项 备份部分数据时，可以在 INSERT 语句最后增加 WHERE 条件 备份表必须以 bak_ 为前缀，并以 _yyyyMMdd 实时日期为后缀。例如 bak_test01_20190409 最好将备份表更改备注，注明备份原因，及最后保留日期，方便清理 我要修改线上数据 > 需要注意的事项 参考：操作线上数据时需要注意什么？ 我正在开发业务功能 > 需要注意的事项 参考：开发需要注意什么？ powered by Gitbook修订时间： 2023-04-03 14:53:25 "},"markdown/mysql/standard/standard-v1.0.html":{"url":"markdown/mysql/standard/standard-v1.0.html","title":"基本规范","keywords":"","body":"建表需要注意什么？ 使用自增 id 作为主键 使用 innodb 引擎 使用统一编码 utf8mb4 尽量将所有列定义为NOT NULL 索引 NULL 列需要额外空间来保存空还是非空，所以要占用更多的空间 进行比较和计算时对于 NULL 值做特别的处理，可能使索引失效 可以使用 0，特殊值，空字符串代替 NULL 所有表和字段都要添加注释，修改字段含义或对字段表示的状态追加时，需要及时更新字段注释 存储业务数据的表，建议不要物理删除，添加字段 is_deleted 进行逻辑删除 尽量避免在表中建立预留字段 预留字段的命名很难做到见名识义 预留字段无法确认存储的数据类型 使用很大的VARCHAR影响表性能 对预留字段类型修改时，会对全表进行锁定 修改字段成本远远大于增加一个字段 适当进行反范式化设计，便于查询和索引优化 字段允许适当冗余，以提高查询性能，但必须考虑数据一致 冗余字段应遵循：不是频繁修改的字段；不是 varchar 超长字段；更不能是 text 字段 正例：商品类目名称使用频率高，字段长度短，名称基本一成不变 可在相关联的表中冗余存储类目名称避免关联查询 尽量做到冷热数据分离，减小表的宽度 MySQL限制每个表最多存储4096列，每行大小不能超过65535个字节 减少磁盘IO，保证热数据的内存缓存命中率 更有效的利用缓存，避免使用SELECT * 这种方式读入无用的冷数据 可以对表进行垂直拆分，将经常一起使用额列放到一个表中 单表行数超过 500 万行或者单表容量超过 2GB ，才推荐进行分库分表 说明：如果预计三年后的数据量根本达不到这个级别，就不要在创建表时就分库分表 500w并不是MySQL数据库的限制，MySQL并不会对单表数据量做限制，限制取决于存储设置和文件系统 可以通过历史数据归档，分库分表等手段来控制 避免在数据库中存储图片、文件等二进制数据 导致物理文件大，影响读取表数据时系统的IO效率 将图片、文件存储到文件服务器中，数据库中仅存储地址信息 如何命名？ 库名与应用名保持一致 特殊字段如四要素需要使用统一命名 字段 命名 字段 命名 姓名 name 身份证号 identity_num 手机号 phone 银行卡号 bank_card_num 表名和字段名，必须使用小写字母或数字，下划线分割，禁止出现数字开头，禁止两个下划线中间只出现数字见名知意，不可超过32字符 MySQL 在 Windows 下不区分大小写，但在 Linux 下默认是区分大小写，因此，数据库名、表名、字段名，都不允许出现任何大写字母，避免节外生枝 正例：aliyun_admin，rdc_config，level3_name 反例：AliyunAdmin，rdcConfig，level_3_name 表名不使用复数名词 表名应该仅仅表示表里面的实体内容，不应该表示实体数量 正例：system_user，repay_order 反例：system_users，repay_orders 表达是与否概念的字段，必须使用 is_xxx 的方式命名，数据类型是 UNSIGNED TINYINT ( 1表示是，0表示否） 禁止使用 MySQL 保留字，如 desc、range、match、delayed 等（请参考MySQL 官方保留字） 主键索引使用 pk_前缀；唯一索引使用 uk_前缀；普通索引使用 idx_前缀 不同表存储相同数据的列（关联列）的列名和列类型必须完全一致 临时库、表名必须以 bak_ 为前缀，并以 _yyyyMMdd 实时日期为后缀。例如 tmp_test01_20190409 备份库、表必须以 bak_ 为前缀，并以 _yyyyMMdd 实时日期为后缀。例如 bak_test01_20190409 如何选择列的类型？ 字段可以使用多种数据类型时，优先考虑数字类型，其次为日期或二进制类型，最后是字符类型 节约数据库表空间、节约索引存储，更重要的是提升检索速度 正例：level TINYINT UNSIGNED NOT NULL DEFAULT 0 COMMENT '逾期等级' 反例：level VARCHAR(2) NOT NULL DEFAULT '' COMMENT '逾期等级' 整数型选择能符合需求的最短列类型，如果为非负数，必须声明 UNSIGNED 正例：id bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键' 反例：id INT UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '主键' 符合需求指可长期满足，不要因为节省长度影响正常业务 扩展：声明整数类型时，可不指定长度，整数类型的长度是固定的 int(3)中的3仅仅代表显示长度，不会限制存储空间 可搭配zerofill关键字进行零补全，但都是用于终端的显示，不影响实际存储的值 列类型 存储空间 取值范围 signed 取值范围 unsigned tinyint 1字节 -128~127 0~255 smallint 2字节 -32768~32767 0~65535 mediumint 3字节 -8388608~8388607 0~16777215 int 4字节 -2147483648~-2147483647 0~4294967295 bigint 8字节 -9223372036854775808~9223372036854775807 0~18446744073709551615 实数类型使用 DECIMAL，禁止使用 FLOAT 和 DOUBLE float 和 double 在存储的时候，存在精度损失的问题，很可能在值的比较时，得到错误的结果 若存储的数据范围超过 decimal 的范围，建议将数据拆成整数和小数分开存储 decimal 占用空间由定义的宽度决定 每4个字节可以存储9个数字，小数点占用一个 禁止使用字符串类型代替日期类型，日期占用空间小，便于查找过滤，有丰富的处理函数 列类型 存储空间 格式 范围 备注 datetime 8字节 YYYY-MM-DD HH:MM:SS[.fraction] 1000-01-01 00:00:00 ~9999-12-31 23:59:59 与时区无关datetime(6)指定6位微秒 timestamp 4字节 YYYY-MM-DD HH:MM:SS[.fraction] 1970-01-01~2038-01-19 受时区影响timestamp(6)指定6位微秒 date 3字节 YYYY-MM-DD 1000-01-01 ~ 9999-12-31 time 3字节 HH:MM:SS[.fraction] -838:59:59 ~ 838:59:59 存储时间点或持续时间time(6)指定6位微秒 使用 TINYINT 代替 ENUM ENUM本身是字符串类型但内部是由整数存储的，最多可以存储65535种不同的枚举值 修改ENUM值需要使用ALTER语句，会导致锁表 ORDER BY操作效率低，需要先转换为字符串才排序 不要使用数值作为ENUM的枚举值，产生混淆 使用 INT UNSIGNED 存储 IPV4 将字符串（15字节）转化为用数字（4字节）存储，节省空间 在存入和读取时使用函数进行转换 SELECT INET_ATON('255.255.255.255'); SELECT INET_NTOA(4294967295); 使用 VARCHAR 时 选择能符合需求的最小长度 VARCHAR(N) 中的 N 表示字符数不是字节数，不预先分配存储空间 符合需求指可长期满足，不要因为节省长度影响正常业务 长度不要超过 5000，如果存储长度大于此值，建议定义字段类型为 text，独立出来一张表，用主键来对应，避免影响其它字段索引效率，或将数据存入OSS 使用 VARBINARY 存储大小写敏感的变长字符串，VARBINARY 默认区分⼤小写，没有字符集概念，速度快 如何建立索引？ 1. 索引列的选择 SELECT、UPDATE、DELETE语句的WHERE从句中的列建立索引，多个同时出现的列建立联合索引提高性能 包含在ORDER BY、GROUP BY、DISTINCT中的列建立索引，多个同时出现的列建立联合索引提高性能 多表JOIN的关联列上建立索引 2. 索引列顺序的选择 联合索引中，索引按照从左到右的顺序来使用的 把区分度最高的列放在联合索引的最左侧，区分度最高的就是主键和唯一索引的列 区分度相差不大时，选择字段长度小的放在左侧 区分度相差不大，且字段长度相差也不大时，将使用更频繁的列放在左侧 3. 避免进入如下误区 宁滥勿缺，认为一个查询就需要建一个索引 宁缺勿滥，认为索引会消耗空间、严重拖慢更新和新增速度 抵制惟一索引，认为业务的惟一性一律需要在应用层通过[先查后插]方式解决。 业务上具有唯一特性的字段，即使是多个字段的组合，也必须建成唯一索引 不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明显的 即使在应用层做了非常完善的校验控制，只要没有唯一索引，根据墨菲定律，必然有脏数据产生 在 VARCHAR 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据 实际文本区分度决定索引长度即可 一般对字符串类型数据，长度为 20 的索引，区分度会高达 90%以上 可以使用 count(distinct left(列名, 索引长度))/count(*)的区分度来确定 每张表索引不要超过5个 索引并不是越多越好，索引可以提高效率也可以降低效率 索引可以增加查询效率，但同样也会降低插入和更新的效率 索引过多，也会增加查询优化器选择查询计划的时间，导致查询效率的降低 禁止给表中每一列都建立单独的索引 避免建立冗余索引和重复索引 重复索引如：primary key(id)、index(id)、unique index(id) MySQL中的主键自动建立非空唯一索引 冗余索引如：index(a,b,c)、index(a,b)、index(a) 对于频繁查询优先考虑使用覆盖索引 频繁查询如查询商品库存 覆盖索引指包含了所有查询字段的索引 不仅仅是WHERE从句GROUP BY从句中的列，也包含SELECT查询的列组合 避免InnoDB表进行索引的二次查询 可以把随机IP变为顺序IO加快查询效率 开发需要注意什么？ 避免数据类型的隐式转换，会导致索引失效 反例： SELECT name FROM users WHERE id='111'; 禁止使用 SELECT * 消耗更多的CPU和IP以及网络带宽资源 无法使用覆盖索引 可以减少表结构变更带来的影响 使用 IN 代替 OR in的值不要超过500个 in 操作可以有效的利用索引 不使用反向查询，如 NOT IN 和 NOT LIKE 禁止使用 ORDER BY RAND() 进行随机排序 会把表中所有符合条件的数据装载到内存中进行排序 消耗大量的CPU和IO及内存资源 推荐在程序中获取一个随机值，然后根据随机值从数据库获取数据 禁止在 WHERE 从句中对列进行函数转换和计算 反例： ...WHERE DATE(gmt_create)='20180101'; 正例： ...WHERE gmt_create>='20180101' AND gmt_create 禁止 SQL 中存放业务逻辑 使用用 UNION ALL 代替 UNION UNION ALL不需要对结果集再进⾏行排序 禁止在数据库中存储明文密码 避免使用子查询，可以把子查询优化为 JOIN 操作 子查询的结果集无法使用索引 子查询会产生临时表操作，如果子查询数据量大则严重影响效率 消耗过多的CPU和IO资源 超过三个表禁止 JOIN，需要 JOIN 的字段，数据类型必须绝对一致 多表关联查询时， 保证被关联的字段需要有索引 每join一个表会多占用一部分内存(join_buffer_size控制) 会产生临时表操作，影响查询效率 MySQL最多允许关联61个表，建议不超过5个 超过 100w 行的批量写操作，要分批多次进行操作 大批量写操作可能会造成严重的主从延迟 binlog日志为row格式时会产生大量的日志 避免产生大事务操作，导致阻塞 减少与数据库的交互次数 数据库更适合处理批量操作 合并多个相同操作到一起，可以提高处理效率 合理拆分复杂的大 SQL 为多个小 SQL MySQL中一个SQL只能只用一个CPU计算 SQL拆分后可以通过并行执行来提高处理效率 如果有 ORDER BY 的场景，请注意利用索引的有序性 order by 最后的字段是组合索引的一部分，并且放在索引组合顺序的最后 避免出现 file_sort 的情况，影响查询性能 正例：where a=? and b=? order by c; ，索引：a_b_c 反例：索引中有范围查找，那么索引有序性无法利用，如：WHERE a>10 ORDER BY b;，索引a_b无法排序。 页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决 禁止在线上数据库做压力测试 禁止从开发环境，测试环境直接连接生产环境数据库 操作线上数据时需要注意什么？ 禁止使用不含有字段列表的 INSERT 语句 UPDATE 少量数据时，先使用 SELECT 将需要更改的数据查出，并在 UPDATE 语句的 WHERE 条件中添加主键限制 UPDATE 大量数据时，先对数据进行备份 禁止单条 SQL 语句同时更新多个表 修改字段时，将多个 ALERT 语句合并为一个执行 若需大批量插入或更新，执行语句很多，建议每隔三四百行添加 SELECT SLEEP(1); 语句 powered by Gitbook修订时间： 2023-04-03 14:53:25 "},"markdown/mysql/practice/cheat-sheet.html":{"url":"markdown/mysql/practice/cheat-sheet.html","title":"常用 SQL","keywords":"","body":"信息统计 需要更改 scheme_name 为自己需要统计的数据库名SELECT TABLE_NAME 表名, TABLE_COMMENT 注释, TABLE_ROWS 数据量, INDEX_LENGTH 索引长度, TABLE_COLLATION 编码集, AVG_ROW_LENGTH 平均行长度, DATA_LENGTH 数据长度, MAX_DATA_LENGTH 最大数据长度, DATA_FREE 空间碎片, CREATE_TIME 创建时间 FROM information_schema.`TABLES` WHERE TABLE_SCHEMA='scheme_name' ORDER BY TABLE_ROWS DESC; 需要更改 scheme_name 为自己需要统计的数据库名 默认过滤备份表及临时表，可自行修改 sql 通过 table_name 限制查询的信息SELECT CONCAT(c.TABLE_NAME,'(',t.TABLE_COMMENT,')') as 表, c.COLUMN_NAME as 列, c.COLUMN_COMMENT 注释, c.COLUMN_TYPE 数据类型, c.IS_NULLABLE as 是否可为空, c.COLUMN_DEFAULT as 默认值, c.EXTRA as 额外信息 FROM INFORMATION_SCHEMA.COLUMNS c LEFT JOIN INFORMATION_SCHEMA.TABLES t ON c.TABLE_NAME = t.TABLE_NAME where c.table_schema ='scheme_name' and c.TABLE_NAME not like '%bak%' and c.TABLE_NAME not like '%tmp_%' order by c.TABLE_NAME,c.ORDINAL_POSITION 性能监控 SHOW FULL PROCESSLIST; 详细说明参考实时连接监控和管理 powered by Gitbook修订时间： 2023-04-03 14:53:25 "},"markdown/mysql/practice/process-manage.html":{"url":"markdown/mysql/practice/process-manage.html","title":"实时连接监控和管理","keywords":"","body":"应用场景 排查 MySQL 相关问题时，需要查看当前到数据库的连接及状态 查看实时的慢查询 执行某查询后，手动终止此查询 如何查看 MySQL 实时连接 可使用如下命令： SHOW PROCESSLIST; 若执行的 SQL 较长导致查询出的 Info 被截取，可使用如下命令查看全部信息： SHOW FULL PROCESSLIST; 该命令等同于如下命令： SELECT * FROM infomation_scheme.PROCESSLIST 实际上就是查询系统表 infomation_scheme.PROCESSLIST 查询结果说明 SHOW PROCESSLIST;查询出的信息如下： 管理员权限可查看全部，其他用户只能查看当前用户的连接 关于 PROCESSLIST 表的字段说明如下：可参考官方文档 字段 解释 id 连接的唯一标识等同于SHOW PROCESSLIST;命令结果中的 ID 列等同于 performance_schema.threads 表的 processlist_id等同于函数 CONNECTION_ID();的返回结果 user 连接的用户 host 主机信息 db 连接的数据库 command 命令类型 time 时长，单位：秒 state 状态 info 执行的语句 如何结束进行中的连接 可以使用 KILL processlist_id 命令结束指定连接，参考官方文档 processlist_id 就是 SHOW PROCESSLIST 查询结果中的 Id 列 powered by Gitbook修订时间： 2023-04-03 14:53:25 "},"markdown/hbase/env.html":{"url":"markdown/hbase/env.html","title":"环境搭建：macOS HBase + Hadoop + Zookeeper ","keywords":"","body":"搭建 Hadoop 环境 1. 下载 hadoop wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.1.2/hadoop-3.1.2.tar.gz tar -zxvf hadoop-3.1.2.tar.gz 2. 配置 hdfs-site.xml vi hadoop-3.1.2/etc/hadoop/hdfs-site.xml 在默认空的 configuration 节点中添加如下内容 dfs.replication 1 dfs.namenode.name.dir file:/xxx/hadoop-3.1.2/data/namenode dfs.datanode.data.dir file:/xxx/hadoop-3.1.2/data/datanode 属性 说明 dfs.replication hdfs 备份个数，默认为 3 dfs.namenode.name.dir namenode 路径 dfs.datanode.data.dir datanode 路径 3. 配置 core-site.xml vi hadoop-3.1.2/etc/hadoop/core-site.xml 在默认空的 configuration 节点中添加如下内容 hadoop.tmp.dir /xxx/hadoop-3.1.2/data/tmp fs.default.name hdfs://localhost:9999 属性 说明 hadoop.tmp.dir 默认为系统的 /tmp，避免自动清理导致一些问题，建议指定自己的文件夹 fs.default.name hdfs 文件系统的访问地址，在 HBase 的配置中需要通过 hbase.rootdir 属性指向这个路径，以便 HBase 使用 hdfs 的文件系统 4. 配置 jdk 环境变量 vi hadoop-3.1.2/etc/hadoop/hadoop-env.sh 找到文件中如下注释的位置，将 export JAVA_HOME 配置正确，记得确保这条配置不要被注释 # The java implementation to use. By default, this environment # variable is REQUIRED on ALL platforms except OS X! export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Home 5. 格式化 hdfs cd hadoop-3.1.2/bin ./hadoop namenode -format 6. 启动和关闭 cd hadoop-3.1.2/sbin ./start-all.sh ./stop-all.sh 7. 相关访问地址 Resource Manager: http://localhost:9870 JobTracker: http://localhost:8088 Specific Node Information: http://localhost:8042 搭建 HBase 环境 1. 下载 HBase wget http://mirror.bit.edu.cn/apache/hbase/2.0.5/hbase-2.0.5-bin.tar.gz tar -zxvf hbase-2.0.5-bin.tar.gz 后期发现：最新的 2.2.0 版本在搭建 phoenix 环境时会遇到问题，故在此建议安装 2.0.5 版本 2. 配置 hbase-env.sh vi hbase-2.0.5/conf/hbase-env.sh 找到文件中如下注释的位置，将 export JAVA_HOME 配置正确，记得确保这条配置不要被注释 # The java implementation to use. Java 1.8+ required. export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_201.jdk/Contents/Home 找到文件中如下属性的位置，设置为 false，系统默认为 true，表示使用 HBase 自带的 Zookeeper，在这里我们使用自建的 Zookeeper，方便排查问题，在之后会演示如何安装对应的 Zookeeper 环境 # Tell HBase whether it should manage it's own instance of ZooKeeper or not. export HBASE_MANAGES_ZK=false 3. 配置 hbase-site.xml vi hbase-2.0.5/conf/hbase-site.xml hbase.rootdir hdfs://localhost:9999/hbase hbase.tmp.dir /xxx/hbase-2.0.5/data/tmp hbase.cluster.distributed true hbase.zookeeper.quorum 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183 hbase.zookeeper.property.dataDir /Users/lishaofei/Environment/hbase-2.2.0/zookeeper zookeeper.znode.parent /hbase/master --> 属性 说明 hbase.rootdir HBase存储文件的地址，使用 Hadoop 中配置的的 HDFS 地址 hbase.tmp.dir 默认为系统的 /tmp，避免自动清理导致一些问题，建议指定自己的文件夹 hbase.cluster.distributed 是否开启分布式集群 hbase.zookeeper.quorum 自建 zookeeper 需设置这个属性，并指定 Zookeeper 的地址 hbase.zookeeper.property.dataDir 使用内置 Zookeeper 时相关文件的存放目录若使用自建 Zookeeper，这个属性不生效，无需配置等同于自建 Zookeeper 的 cfg 配置中的 dataDir 指定目录 zookeeper.znode.parent 在 Zookeeper 中 HBase 相关节点的根目录 注释掉的属性不需要配置 hbase.zookeeper.property.dataDir 由于使用了自建 Zookeeper 所以不用配置 网上一些教程将 zookeeper.znode.parent 设置为 /hbase/master，这样在使用 JavaAPI 的判断表是否存在时将抛出如下异常，原因是默认的 HBase 会在 Zookeeper 下建立节点 /hbase/table/hbase:meta，但是当日加入上述配置时，这个节点就建立为 /hbase/master/table/hbase:meta 导致找不到对应的表元数据 No meta znode available, details=row '123' on table 'hbase:meta' at null 4. 启动，关闭 cd hbase-2.0.5/bin ./start-hbase.sh ./stop-hbase.sh 5. 进入 Hbase Shell 并查看状态 cd hbase-2.0.5/bin ./hbase shell hbase(main):001:0> status 1 active master, 0 backup masters, 1 servers, 0 dead, 2.0000 average load Took 0.4010 seconds 进入 zk 可使用：./bin/hbase zkcli 6. 问题处理 出现如下警告请直接忽略 WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 在 HBase Shell 命令行中执行 status 出现如下错误，到 hadoop/bin 目录下执行 ./hadoop dfsadmin -safemode leave ERROR: org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet 如下三个问题 HBase 启动后 HMaster 节点自己挂掉 无法使用 Stop 关闭 HBase，jps 查看会留下 HRegionServer 节点没有被关闭 进入 Hbase Shell 执行命令报错 ERROR: KeeperErrorCode = NoNode for /hbase/master； 解决方案 进入 HBase 目录下的 logs 文件夹下，查看 master.log 相关日志定位问题（出现任何问题先查这个） 如果是配置问题，请在更改配置后，重启前，删除 Zookeeper 指定的 dataDir 目录下的 version-2 文件夹、删除 HBase 指定的 tmp 目录的内容再重启 还是解决不了，则：关闭所有 java 进程、清理 Hadoop tmp 文件下的内容、重新格式化HDFS、清理 HBase tmp 目录的内容、删除 Zookeeper 指定的 dataDir 目录下的 version-2 文件夹、删除 HBase 目录下的 logs 文件夹中的所有日志，重启，并观察master.log 7. 访问 http://localhost:16010 自建 zookeeper 伪分布式集群 1. 下载 wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.5.5/apache-zookeeper-3.5.5-bin.tar.gz tar -zxvf apache-zookeeper-3.5.5-bin.tar.gz 2. 伪分布式配置 复制三份配置文件 cd apache-zookeeper-3.5.5-bin/conf cp zoo_sample.cfg zoo1.cfg cp zoo_sample.cfg zoo2.cfg cp zoo_sample.cfg zoo3.cfg 修改 zoo1.cfg 的如下两个属性 dataDir=/xxx/apache-zookeeper-3.5.5-bin/data/zk1 clientPort=2181 修改 zoo2.cfg 的如下两个属性 dataDir=/xxx/apache-zookeeper-3.5.5-bin/data/zk2 clientPort=2182 修改 zoo3.cfg 的如下两个属性 dataDir=/xxx/apache-zookeeper-3.5.5-bin/data/zk3 clientPort=2183 在三个配置文件末尾均加入如下内容 server.1=127.0.0.1:2888:3888 server.2=127.0.0.1:2889:3889 server.3=127.0.0.1:2890:3890 3. 启动和关闭 创建启动/关闭脚本，可以手动一个一个启动，这里只是为了方便操作 cd apache-zookeeper-3.5.5-bin # 创建一键启动脚本 vi start-all.sh # 粘贴如下内容并保存 bin/zkServer.sh start conf/zoo1.cfg bin/zkServer.sh start conf/zoo2.cfg bin/zkServer.sh start conf/zoo3.cfg # 保存后赋权 chmod 777 start-all.sh # 创建一键关闭脚本 vi stop-all.sh # 粘贴如下内容并保存 bin/zkServer.sh stop conf/zoo1.cfg bin/zkServer.sh stop conf/zoo2.cfg bin/zkServer.sh stop conf/zoo3.cfg # 保存后赋权 chmod 777 stop-all.sh 启动/关闭 cd apache-zookeeper-3.5.5-bin ./start-all.sh ./stop-all.sh powered by Gitbook修订时间： 2023-04-03 14:53:25 "},"markdown/hbase/optimize.html":{"url":"markdown/hbase/optimize.html","title":"优化策略","keywords":"","body":"HBase 优化策略 什么导致 HBase 性能下降 JVM 内存分配及 GC 回收策略 与 HBase 运行机制相关的部分配置不合理 表结构设计及使用方式不合理 HBase 概念 HBase 写入时，MemStore 达到一定的大小是会 flush 到 HFile HFile 小文件太多时会执行 compact 操作进行合并 minor compaction 选取小的、相邻的 StoreFile 合并成一个大的 StoreFile major compaction 将所有 StoreFile 合并成一个 StoreFile，清理失效数据(被删除的数据、TTL 过期数据、版本号超过设定阈值的数据) Compact 何时进行 MemStore flush 到 HFile 时 用户执行 shell 命令 compact、major_compact 或调用相关 API 时 HBase 后台线程周期性触发检查 当 Region 的大小达到某一阈值之后，会执行 split 操作，切分成两个 Region 服务端配置优化 JVM 设置与 GC 设置 尽量将 RegionServer 内存划分大一些，避免频繁 FullGC hbase-site.xml 属性配置 hbase.regionserver.handler.count rpc 请求的线程数量，默认为 10，提升此配置，有利于 RegionServer 接受更多的请求，但设置过多可能导致内存占用大，频繁 FullGC，甚至出现内存溢出 hbase.hregion.max.filesize 当 region 的大小大于设定值后，hbase 就会开始 split，默认为10G，可根据存储内容适当调整 hbase.hregion.majorcompaction major compaction 的执行周期，默认为 1 天，建议设置为 0，禁止自动执行，生产环境低峰期手动执行或脚本执行，不影响正常业务 hbase.hstore.compaction.min 一个 store 中的 StoreFile 总数超过此值，则触发默认的 compact 操作，默认 3 hbase.hstore.compaction.max 一次最多合并多少个 StoreFile，如果 StoreFile 较大，应该适当减小此值，避免合并时内存溢出 hbase.hstore.blockingStoreFiles 一个 Region 中的 Store 内超过 xx 个 StoreFile 时，则 block 所有的写请求进行 compact hfile.block.cache.size RegionServer 的 block cache 的内存大小限制 hbase.hregion.memstore.flush.size memstore 超过此值会被 flush hbase.hregion.memstore.block.multiplier 如果 memstore 的内存大小超过 flush.size * multiplier 的值，会 block 该 memstore 的写操作 总体策略：避免自动的 major compaction 和 split 操作，通过开发脚本在业务低峰时进行手动触发 常用优化策略(建表、RowKey 设计等) 预先分区 创建 HBase 表时默认在一个 RegionServer 上创建一个 Region，写数据时，导致该 Region 达到一定大小是，进行 split 操作，分为连个 Region，进行负载均衡，split 十分耗时，且会造成 Region 无法访问 创建 HBase 表时预先创建一些空的 Regions，并且规定每个 Region 存储的 RowKey 的范围 通过预先分区还可解决数据倾斜的问题 RowKey 优化 利用 HBase 默认排序的特点，将一起访问的数据放到一起 防止热点问题，避免使用时序或单调的递增递减 Column 优化 列族的名称和列的描述名称尽量简短 同一张表的 ColumnFamily 不要超过 3 个 Schema 优化 宽表：列多行少 高表：列少行多 HBase 读写性能优化(Java API 开发过程) 写优化策略 同步批量提交 or 异步批量提交，异步可能会丢失数据，在业务允许的情况下，开启异步提交以提高性能 关闭 WAL 或采用异步 WAL，损失数据完整性，提高吞吐量 读优化策略 客户端：Scan 缓存设置，批量获取 服务端：BlockCache 配置是否合理，HFile 是否过多 表结构设计问题 powered by Gitbook修订时间： 2023-04-03 14:53:25 "},"markdown/hbase/hbase_shell.html":{"url":"markdown/hbase/hbase_shell.html","title":"HBase Shell 实操","keywords":"","body":" 查看表目录 hbase(main):006:0> list TABLE 0 row(s) in 0.0150 seconds => [] 创建表 hbase(main):007:0> create 'FileTable','fileInfo','saveInfo' 0 row(s) in 1.3810 seconds => Hbase::Table - FileTable hbase(main):008:0> list TABLE FileTable 1 row(s) in 0.0120 seconds => [\"FileTable\"] hbase(main):009:0> desc 'FileTable' Table FileTable is ENABLED FileTable COLUMN FAMILIES DESCRIPTION {NAME => 'fileInfo', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_ BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZ E => '65536', REPLICATION_SCOPE => '0'} {NAME => 'saveInfo', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_ BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZ E => '65536', REPLICATION_SCOPE => '0'} 2 row(s) in 0.0350 seconds 增加列族 hbase(main):010:0> alter 'FileTable','cf' Updating all regions with the new schema... 1/1 regions updated. Done. 0 row(s) in 1.9260 seconds hbase(main):011:0> desc 'FileTable' Table FileTable is ENABLED FileTable COLUMN FAMILIES DESCRIPTION {NAME => 'cf', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => ' 65536', REPLICATION_SCOPE => '0'} {NAME => 'fileInfo', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_ BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZ E => '65536', REPLICATION_SCOPE => '0'} {NAME => 'saveInfo', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_ BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZ E => '65536', REPLICATION_SCOPE => '0'} 3 row(s) in 0.0180 seconds 删除列族 hbase(main):013:0> alter 'FileTable',{NAME=>'cf',METHOD=>'delete'} Updating all regions with the new schema... 1/1 regions updated. Done. 0 row(s) in 2.1460 seconds hbase(main):015:0> desc 'FileTable' Table FileTable is ENABLED FileTable COLUMN FAMILIES DESCRIPTION {NAME => 'fileInfo', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_ BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZ E => '65536', REPLICATION_SCOPE => '0'} {NAME => 'saveInfo', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_ BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZ E => '65536', REPLICATION_SCOPE => '0'} 2 row(s) in 0.0180 seconds 增加两条记录 hbase(main):016:0> put 'FileTable','rowkey1','fileInfo:name','file1.txt' 0 row(s) in 0.0640 seconds hbase(main):017:0> put 'FileTable','rowkey1','fileInfo:type','txt' 0 row(s) in 0.0030 seconds hbase(main):018:0> put 'FileTable','rowkey1','fileInfo:size','1024' 0 row(s) in 0.0060 seconds hbase(main):019:0> put 'FileTable','rowkey1','saveInfo:path','/home' 0 row(s) in 0.0110 seconds hbase(main):020:0> put 'FileTable','rowkey1','saveInfo:creator','tom' 0 row(s) in 0.0030 seconds hbase(main):021:0> put 'FileTable','rowkey2','fileInfo:name','file2.txt' 0 row(s) in 0.0030 seconds hbase(main):022:0> put 'FileTable','rowkey2','fileInfo:type','txt' 0 row(s) in 0.0030 seconds hbase(main):023:0> put 'FileTable','rowkey2','fileInfo:size','2048' 0 row(s) in 0.0030 seconds hbase(main):024:0> put 'FileTable','rowkey2','saveInfo:path','/home' 0 row(s) in 0.0040 seconds hbase(main):025:0> put 'FileTable','rowkey2','saveInfo:creator','jack' 0 row(s) in 0.0040 seconds 查看表中的记录条数 hbase(main):026:0> count 'FileTable' 2 row(s) in 0.0240 seconds => 2 查看一行数据 hbase(main):027:0> get 'FileTable','rowkey1' COLUMN CELL fileInfo:name timestamp=1562049913828, value=file1.txt fileInfo:size timestamp=1562049931731, value=1024 fileInfo:type timestamp=1562049924426, value=txt saveInfo:creator timestamp=1562049958992, value=tom saveInfo:path timestamp=1562049946815, value=/home 1 row(s) in 0.0170 seconds 查看一行指定列族数据 hbase(main):028:0> get 'FileTable','rowkey1','fileInfo' COLUMN CELL fileInfo:name timestamp=1562049913828, value=file1.txt fileInfo:size timestamp=1562049931731, value=1024 fileInfo:type timestamp=1562049924426, value=txt 1 row(s) in 0.0170 seconds 查询表中数据 hbase(main):001:0> scan 'FileTable' ROW COLUMN+CELL rowkey1 column=fileInfo:name, timestamp=1562049913828, value=file1.txt rowkey1 column=fileInfo:size, timestamp=1562049931731, value=1024 rowkey1 column=fileInfo:type, timestamp=1562049924426, value=txt rowkey1 column=saveInfo:creator, timestamp=1562049958992, value=tom rowkey1 column=saveInfo:path, timestamp=1562049946815, value=/home rowkey2 column=fileInfo:name, timestamp=1562049974675, value=file2.txt rowkey2 column=fileInfo:size, timestamp=1562049998454, value=2048 rowkey2 column=fileInfo:type, timestamp=1562049983426, value=txt rowkey2 column=saveInfo:creator, timestamp=1562050020654, value=jack rowkey2 column=saveInfo:path, timestamp=1562050007276, value=/home 2 row(s) in 0.1720 seconds hbase(main):002:0> scan 'FileTable',{COLUMN=>'fileInfo:name'} ROW COLUMN+CELL rowkey1 column=fileInfo:name, timestamp=1562049913828, value=file1.txt rowkey2 column=fileInfo:name, timestamp=1562049974675, value=file2.txt 2 row(s) in 0.0130 seconds hbase(main):004:0> scan 'FileTable',{STARTROW=>'rowkey1',LIMIT=>1,VERSIONS=>1} ROW COLUMN+CELL rowkey1 column=fileInfo:name, timestamp=1562049913828, value=file1.txt rowkey1 column=fileInfo:size, timestamp=1562049931731, value=1024 rowkey1 column=fileInfo:type, timestamp=1562049924426, value=txt rowkey1 column=saveInfo:creator, timestamp=1562049958992, value=tom rowkey1 column=saveInfo:path, timestamp=1562049946815, value=/home 1 row(s) in 0.0100 seconds 删除一列数据 hbase(main):005:0> delete 'FileTable','rowkey1','fileInfo:size' 0 row(s) in 0.0450 seconds hbase(main):006:0> get 'FileTable','rowkey1' COLUMN CELL fileInfo:name timestamp=1562049913828, value=file1.txt fileInfo:type timestamp=1562049924426, value=txt saveInfo:creator timestamp=1562049958992, value=tom saveInfo:path timestamp=1562049946815, value=/home 1 row(s) in 0.0360 seconds 删除一行数据 hbase(main):007:0> deleteall 'FileTable','rowkey1' 0 row(s) in 0.0030 seconds hbase(main):008:0> get 'FileTable','rowkey1' COLUMN CELL 0 row(s) in 0.0020 seconds 删除表，需要先禁用 hbase(main):009:0> disable 'FileTable' 0 row(s) in 2.3310 seconds hbase(main):010:0> is_enabled 'FileTable' false 0 row(s) in 0.0050 seconds hbase(main):012:0> is_disabled 'FileTable' true 0 row(s) in 0.0050 seconds hbase(main):013:0> drop 'FileTable' 0 row(s) in 1.2600 seconds hbase(main):015:0> exists 'FileTable' Table FileTable does not exist 0 row(s) in 0.0040 seconds hbase(main):017:0> list TABLE 0 row(s) in 0.0170 seconds => [] powered by Gitbook修订时间： 2023-04-03 14:53:25 "}}